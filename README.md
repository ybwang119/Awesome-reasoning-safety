# Awesome-reasoning-safety
## Attacks
[Adversarial Reasoning at Jailbreaking Time](https://arxiv.org/abs/2502.01633) (02/03, 2025)

[H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking](https://arxiv.org/abs/2502.12893v1) (02/18, 2025)

[THE DARK DEEP SIDE OF DEEPSEEK: FINE-TUNING ATTACKS AGAINST THE SAFETY ALIGNMENT OF COT-ENABLED MODELS](https://arxiv.org/abs/2502.01225) (02/03, 2025)

[OVERTHINK: Slowdown Attacks on Reasoning LLMs](https://arxiv.org/abs/2502.02542) (02/05, 2025)

[Safechain: Safety of language models with long chain-of-thought reasoning capabilities](https://arxiv.org/abs/2502.12025) (02/17, 2025)

[The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1](https://arxiv.org/abs/2502.12659v3) (02/27, 2025)

[Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/abs/2503.00555) (03/01, 2025)

## Defenses
GuardReasoner: Towards Reasoning-based LLM Safeguards


## Studies
TRADING INFERENCE-TIME COMPUTE FOR ADVERSARIAL ROBUSTNESS.
## Surveys, Datasets and Benchmarks
